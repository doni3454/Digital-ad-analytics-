{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LASSO - DA - Python Exercise 3- Aratrika (Doni) Rath.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS2QBdKipHG6",
        "colab_type": "text"
      },
      "source": [
        "This project deals with works with sales data for Bobo Bars. #purchases is the raw count of sales we saw for a calendar year in a particular are of the U.S. All of the other columns refer to Census data. The objective is to predict sales from census data and fugureout a baseline sales number. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yoi4Vi9ybzb_",
        "colab_type": "text"
      },
      "source": [
        "Mounting the drive "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztzuakqIkTbG",
        "colab_type": "code",
        "outputId": "44a47671-acfd-4b42-ec16-a11b4187355a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weJ8yTqdbPpc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd \n",
        "alldata=pd.read_csv('/content/gdrive/My Drive/Academics/Sem 3/Data/finalmaster-ratios.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBf9CB_lb4dv",
        "colab_type": "code",
        "outputId": "15d0683d-9d15-4001-e2ce-42498036f26e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "#Printing all data \n",
        "print(alldata)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     # Purchases  B01001001   B01001002  ...   B19001015  B19001016  B19001017\n",
            "0             22     206252  469.226965  ...   36.440765  23.446284  21.197485\n",
            "1              7      61399  486.538869  ...   33.000508  33.169741  24.792689\n",
            "2              3      73170  489.859232  ...   20.125056  11.890525  16.537397\n",
            "3             94     251724  505.585483  ...   43.731067  38.851729  40.427349\n",
            "4              0      37382  495.586111  ...   36.118530  31.603714  19.648989\n",
            "..           ...        ...         ...  ...         ...        ...        ...\n",
            "727            4      38106  482.312497  ...   27.706620  19.347445  15.033032\n",
            "728            3      32531  511.665796  ...  106.653459  84.530296  76.027737\n",
            "729            3      38761  493.511519  ...   49.390327  47.814769  31.031648\n",
            "730            1      28359  496.773511  ...   29.939545  24.085980  19.192016\n",
            "731          507    5861000  484.835011  ...   40.966917  44.925312  52.695060\n",
            "\n",
            "[732 rows x 190 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUSEdqeLcJJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import pandas\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LassoLarsCV\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltEqaT0Vcjtg",
        "colab_type": "code",
        "outputId": "cb677a51-54ca-48c3-c40f-5e8c5028786b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "\n",
        "#Getting all variable names/ column names from the dataset \n",
        "allvariablenames = list(alldata.columns.values)\n",
        "print(allvariablenames)\n",
        "\n",
        "# Excluding first 8 variable / column names since they are valid predictors \n",
        "listofallpredictors = allvariablenames[8:]\n",
        "print(len(listofallpredictors), listofallpredictors)\n",
        "\n",
        "#load predictors into dataframe\n",
        "predictors = alldata[listofallpredictors]  \n",
        "print (\"These are the predictors\", predictors)\n",
        "\n",
        "#load target into dataframe\n",
        "target = alldata['# Purchases'] \n",
        "print(\"This is the target\", target)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['# Purchases', 'B01001001', 'B01001002', 'B01001003', 'B01001004', 'B01001005', 'B01001006', 'B01001007', 'B01001008', 'B01001009', 'B01001010', 'B01001011', 'B01001012', 'B01001013', 'B01001014', 'B01001015', 'B01001016', 'B01001017', 'B01001018', 'B01001019', 'B01001020', 'B01001021', 'B01001022', 'B01001023', 'B01001024', 'B01001025', 'B01001026', 'B01001027', 'B01001028', 'B01001029', 'B01001030', 'B01001031', 'B01001032', 'B01001033', 'B01001034', 'B01001035', 'B01001036', 'B01001037', 'B01001038', 'B01001039', 'B01001040', 'B01001041', 'B01001042', 'B01001043', 'B01001044', 'B01001045', 'B01001046', 'B01001047', 'B01001048', 'B01001049', 'B02001001', 'B02001002', 'B02001003', 'B02001004', 'B02001005', 'B02001006', 'B02001007', 'B02001008', 'B02001009', 'B02001010', 'B12001001', 'B12001002', 'B12001003', 'B12001004', 'B12001005', 'B12001006', 'B12001007', 'B12001008', 'B12001009', 'B12001010', 'B12001011', 'B12001012', 'B12001013', 'B12001014', 'B12001015', 'B12001016', 'B12001017', 'B12001018', 'B12001019', 'B13014001', 'B13014002', 'B13014003', 'B13014004', 'B13014005', 'B13014006', 'B13014007', 'B13014008', 'B13014009', 'B13014010', 'B13014011', 'B13014012', 'B13014013', 'B13014014', 'B13014015', 'B13014016', 'B13014017', 'B13014018', 'B13014019', 'B13014020', 'B13014021', 'B13014022', 'B13014023', 'B13014024', 'B13014025', 'B13014026', 'B13014027', 'B13015001', 'B13015002', 'B13015003', 'B13015004', 'B13015005', 'B13015006', 'B13015007', 'B13015008', 'B13015009', 'B13015010', 'B13015011', 'B13015012', 'B13015013', 'B13015014', 'B13015015', 'B13016001', 'B13016002', 'B13016003', 'B13016004', 'B13016005', 'B13016006', 'B13016007', 'B13016008', 'B13016009', 'B13016010', 'B13016011', 'B13016012', 'B13016013', 'B13016014', 'B13016015', 'B13016016', 'B13016017', 'B15002001', 'B15002002', 'B15002003', 'B15002004', 'B15002005', 'B15002006', 'B15002007', 'B15002008', 'B15002009', 'B15002010', 'B15002011', 'B15002012', 'B15002013', 'B15002014', 'B15002015', 'B15002016', 'B15002017', 'B15002018', 'B15002019', 'B15002020', 'B15002021', 'B15002022', 'B15002023', 'B15002024', 'B15002025', 'B15002026', 'B15002027', 'B15002028', 'B15002029', 'B15002030', 'B15002031', 'B15002032', 'B15002033', 'B15002034', 'B15002035', 'B19001001', 'B19001002', 'B19001003', 'B19001004', 'B19001005', 'B19001006', 'B19001007', 'B19001008', 'B19001009', 'B19001010', 'B19001011', 'B19001012', 'B19001013', 'B19001014', 'B19001015', 'B19001016', 'B19001017']\n",
            "182 ['B01001008', 'B01001009', 'B01001010', 'B01001011', 'B01001012', 'B01001013', 'B01001014', 'B01001015', 'B01001016', 'B01001017', 'B01001018', 'B01001019', 'B01001020', 'B01001021', 'B01001022', 'B01001023', 'B01001024', 'B01001025', 'B01001026', 'B01001027', 'B01001028', 'B01001029', 'B01001030', 'B01001031', 'B01001032', 'B01001033', 'B01001034', 'B01001035', 'B01001036', 'B01001037', 'B01001038', 'B01001039', 'B01001040', 'B01001041', 'B01001042', 'B01001043', 'B01001044', 'B01001045', 'B01001046', 'B01001047', 'B01001048', 'B01001049', 'B02001001', 'B02001002', 'B02001003', 'B02001004', 'B02001005', 'B02001006', 'B02001007', 'B02001008', 'B02001009', 'B02001010', 'B12001001', 'B12001002', 'B12001003', 'B12001004', 'B12001005', 'B12001006', 'B12001007', 'B12001008', 'B12001009', 'B12001010', 'B12001011', 'B12001012', 'B12001013', 'B12001014', 'B12001015', 'B12001016', 'B12001017', 'B12001018', 'B12001019', 'B13014001', 'B13014002', 'B13014003', 'B13014004', 'B13014005', 'B13014006', 'B13014007', 'B13014008', 'B13014009', 'B13014010', 'B13014011', 'B13014012', 'B13014013', 'B13014014', 'B13014015', 'B13014016', 'B13014017', 'B13014018', 'B13014019', 'B13014020', 'B13014021', 'B13014022', 'B13014023', 'B13014024', 'B13014025', 'B13014026', 'B13014027', 'B13015001', 'B13015002', 'B13015003', 'B13015004', 'B13015005', 'B13015006', 'B13015007', 'B13015008', 'B13015009', 'B13015010', 'B13015011', 'B13015012', 'B13015013', 'B13015014', 'B13015015', 'B13016001', 'B13016002', 'B13016003', 'B13016004', 'B13016005', 'B13016006', 'B13016007', 'B13016008', 'B13016009', 'B13016010', 'B13016011', 'B13016012', 'B13016013', 'B13016014', 'B13016015', 'B13016016', 'B13016017', 'B15002001', 'B15002002', 'B15002003', 'B15002004', 'B15002005', 'B15002006', 'B15002007', 'B15002008', 'B15002009', 'B15002010', 'B15002011', 'B15002012', 'B15002013', 'B15002014', 'B15002015', 'B15002016', 'B15002017', 'B15002018', 'B15002019', 'B15002020', 'B15002021', 'B15002022', 'B15002023', 'B15002024', 'B15002025', 'B15002026', 'B15002027', 'B15002028', 'B15002029', 'B15002030', 'B15002031', 'B15002032', 'B15002033', 'B15002034', 'B15002035', 'B19001001', 'B19001002', 'B19001003', 'B19001004', 'B19001005', 'B19001006', 'B19001007', 'B19001008', 'B19001009', 'B19001010', 'B19001011', 'B19001012', 'B19001013', 'B19001014', 'B19001015', 'B19001016', 'B19001017']\n",
            "These are the predictors      B01001008  B01001009  B01001010  ...   B19001015  B19001016  B19001017\n",
            "0     6.734480   6.225394  19.432539  ...   36.440765  23.446284  21.197485\n",
            "1    18.192479  13.534422  21.466148  ...   33.000508  33.169741  24.792689\n",
            "2     7.243406   7.380074  16.933169  ...   20.125056  11.890525  16.537397\n",
            "3     7.035483   7.686991  25.790151  ...   43.731067  38.851729  40.427349\n",
            "4     6.580707   7.062222  17.334546  ...   36.118530  31.603714  19.648989\n",
            "..         ...        ...        ...  ...         ...        ...        ...\n",
            "727  19.367029  22.673595  30.100247  ...   27.706620  19.347445  15.033032\n",
            "728   6.086502   4.949125  20.411300  ...  106.653459  84.530296  76.027737\n",
            "729   8.694306   8.900699  14.834499  ...   49.390327  47.814769  31.031648\n",
            "730   5.042491   5.148277  22.250432  ...   29.939545  24.085980  19.192016\n",
            "731   6.963999   6.359666  19.319911  ...   40.966917  44.925312  52.695060\n",
            "\n",
            "[732 rows x 182 columns]\n",
            "This is the target 0       22\n",
            "1        7\n",
            "2        3\n",
            "3       94\n",
            "4        0\n",
            "      ... \n",
            "727      4\n",
            "728      3\n",
            "729      3\n",
            "730      1\n",
            "731    507\n",
            "Name: # Purchases, Length: 732, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9A1tmyWDfBQI",
        "colab_type": "code",
        "outputId": "26b64ba7-4f40-42bf-83c1-24391b4807ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Splitting the data \n",
        "pred_train, pred_test, tar_train, tar_test = train_test_split(predictors, target, test_size=.3, random_state=123)\n",
        "\n",
        "x_train,y_train = pred_train, tar_train\n",
        "\n",
        "model = LassoLarsCV(cv=10, precompute= False)\n",
        "print(\"this is the LASSO model\", model)\n",
        "\n",
        "# fit our newly created model with our training data\n",
        "reg_train = model.fit(x_train,y_train)\n",
        "print (\"This is the regression of training data\", reg_train )\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "this is the LASSO model LassoLarsCV(copy_X=True, cv=10, eps=2.220446049250313e-16, fit_intercept=True,\n",
            "            max_iter=500, max_n_alphas=1000, n_jobs=None, normalize=True,\n",
            "            positive=False, precompute=False, verbose=False)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.496e+00, with an active set of 5 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.098e+00, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=7.329e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.051e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 17 iterations, alpha=5.739e-01, previous alpha=5.739e-01, with an active set of 14 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.100e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.867e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=2.050e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=2.013e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=1.986e-01, previous alpha=1.960e-01, with an active set of 37 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.365e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.008e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.144e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.642e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.644e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=5.626e-01, previous alpha=5.354e-01, with an active set of 13 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.439e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.037e+00, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 15 iterations, alpha=7.201e-01, previous alpha=7.200e-01, with an active set of 12 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=4.546e-02, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=4.539e-02, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 116 iterations, alpha=3.652e-02, previous alpha=3.599e-02, with an active set of 99 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.572e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.132e+00, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.640e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=3.863e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=3.863e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.560e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.932e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=1.737e-01, with an active set of 41 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 64 iterations, alpha=1.702e-01, previous alpha=1.687e-01, with an active set of 45 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.644e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.178e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.316e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=8.155e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=3.203e-01, previous alpha=3.135e-01, with an active set of 25 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.100e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "This is the regression of training data LassoLarsCV(copy_X=True, cv=10, eps=2.220446049250313e-16, fit_intercept=True,\n",
            "            max_iter=500, max_n_alphas=1000, n_jobs=None, normalize=True,\n",
            "            positive=False, precompute=False, verbose=False)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=6.523e-02, with an active set of 60 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=6.523e-02, with an active set of 60 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 68 iterations, alpha=6.435e-02, previous alpha=6.382e-02, with an active set of 61 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.147e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.014e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.010e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.693e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.099e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.948e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=3.137e-01, previous alpha=3.116e-01, with an active set of 27 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.644e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=2.822e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 9.125e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 37 iterations, alpha=2.062e-01, previous alpha=1.984e-01, with an active set of 34 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.372e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.825e-01, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.732e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=5.285e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=5.186e-01, previous alpha=5.185e-01, with an active set of 15 regressors.\n",
            "  ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGlq9LwXhmZZ",
        "colab_type": "code",
        "outputId": "265c1016-575b-47e5-d2fa-cdb5827a1a94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "#Extracting coefficients to see which preditcors are most significant\n",
        "#build coefficent chart\n",
        "\n",
        "predictors_model=pd.DataFrame(listofallpredictors)\n",
        "predictors_model.columns = ['label']\n",
        "predictors_model['coeff'] = model.coef_\n",
        "\n",
        "for index, row in predictors_model.iterrows():\n",
        "    if row['coeff'] > 0:\n",
        "      print(row['coeff'], row.values)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8558761066941788 ['B01001014' 0.8558761066941788]\n",
            "2.5053482381631653 ['B01001036' 2.5053482381631653]\n",
            "0.8892493223320962 ['B01001037' 0.8892493223320962]\n",
            "1.5316387928880384 ['B01001038' 1.5316387928880384]\n",
            "0.41252295298457853 ['B02001005' 0.41252295298457853]\n",
            "0.48004105312075906 ['B13014026' 0.48004105312075906]\n",
            "0.6978957445987839 ['B13014027' 0.6978957445987839]\n",
            "875149895.329212 ['B13016001' 875149895.329212]\n",
            "1.4834348068681533 ['B19001017' 1.4834348068681533]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJPs5hUojXN-",
        "colab_type": "text"
      },
      "source": [
        "Q1. In your own words, explain what the above lines of code are doing. Why am I doing it? Explain each line.\n",
        "\n",
        "Overall idea of this code is to map the predictor names to coefficients found from the regression model.\n",
        "\n",
        "In the first line, we are converting the list of all predictors names to a dataframe. In the next line, we are initializing the column names of the dataframe to 'label' because the dataframe only contains the labels of predictors. \n",
        "Then, in the third line, we are adding a new column called 'coeff' which has the values coming from the LASSO regression model. \n",
        "Finally, we are using for loop to iterate over all the coefficients for all the predictors and only printing those predictors whose coefficent is greater than zero. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2KZqjMjkusL",
        "colab_type": "text"
      },
      "source": [
        "Q2.  Interpret each variable intuitively. What Census variables most predict sales? What does that mean, practically? Here's what I'd put for the example above. \"In areas where there are more females aged 30-34, we sell more Bobo Bars.\"\n",
        "\n",
        "The census variables that most predict sales are the follows. \n",
        "\n",
        "B01001014 - In areas where we have a male population between 40 - 44 years we sell more Bobo Bars \n",
        "\n",
        "B01001036 - In areas where we have a female population between 30-34 years we sell more Bobo Bars \n",
        "\n",
        "B01001037 - In areas where we have a female population between 35-39 years we sell more Bobo Bars\n",
        "\n",
        "B01001038 - In areas where we have a female population between 40-44 years we sell more Bobo Bars\n",
        "\n",
        "B02001005 - In areas where we have an asian population we sell more Bobo Bars\n",
        "\n",
        "B13014026 - In areas where we have women between the ages of 15-50 years, had a birth, and hold a bachelor's degree in education, sells more Bobo bars. \n",
        "\n",
        "B13014027 - In areas where we have women between the ages of 15-50 years, had a birth, and hold a graduate or professional degree in education, sells more Bobo bars.\n",
        "\n",
        "B13016001 - In areas where women are between 15 - 50 years and had ave had a kid in the last 12 months, sell more Bobo bars\n",
        "\n",
        "B19001017 - In areas were Household income is $200,000 or more in the last 12 months, we sell more Bobo bars. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guiPFh6moKjj",
        "colab_type": "text"
      },
      "source": [
        "Q3. If I had to report only two census variables to my boss that most steeply predicted sales, what would those be? \n",
        "\n",
        "B13016001 - In areas where women are between 15 - 50 years and had have had a kid in the last 12 months. \n",
        "\n",
        "B01001036 - In areas where we have a female population between 30-34 years. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLW2k4lFoqY0",
        "colab_type": "code",
        "outputId": "cb8b8342-b38f-4b07-ae77-67bf9b48e55e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "# Next, lets take a look at the mean squared error for the training and training set:\n",
        "train_error = mean_squared_error(tar_train, model.predict(pred_train))\n",
        "print ('training data MSE')\n",
        "print(train_error)\n",
        "\n",
        "# Run the code above, then do the same thing for your test sets (2) and (4) above. Compare the two outputs.\n",
        "test_error = mean_squared_error(tar_test, model.predict(pred_test))\n",
        "print ('testing data MSE')\n",
        "print(test_error)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training data MSE\n",
            "22025.491066757\n",
            "testing data MSE\n",
            "41549.54803776253\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YqfnmmmpHQk",
        "colab_type": "text"
      },
      "source": [
        "QUESTION #4: Are the training and text set mean squared errors similar? What does that mean practically? Think back to your stats class, or Google it!\n",
        "\n",
        "It is a known fact that smaller the Mean squared error value, better is your regression fit.\n",
        "\n",
        "Comparing the above two mean squared, it is clear that the testing data has twice the  mean squared error of training data. Therefore, training data is better in terms of prediction. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAuepELcrguo",
        "colab_type": "code",
        "outputId": "e8ec126b-69ff-463d-da67-689c1ba9d6e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Finding R - squared for teh training set \n",
        "#For training \n",
        "rsquared_train=model.score(pred_train,tar_train)\n",
        "print ('training data R-square')\n",
        "print(rsquared_train)\n",
        "\n",
        "#For test \n",
        "rsquared_test=model.score(pred_test,tar_test)\n",
        "print ('testing data R-square')\n",
        "print(rsquared_test)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training data R-square\n",
            "0.2400221219784492\n",
            "testing data R-square\n",
            "0.1758628512005107\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0sh_lglsUDS",
        "colab_type": "text"
      },
      "source": [
        "Q4 continued \n",
        "\n",
        "The larger the R squared better the regression fit. Therefore training data has a slightly better chance at predicting future sales.\n",
        "\n",
        "To summarize, both R sqaured and mean sqaure error indicate that training data is a better indicator for predicting future sales.  Since, we used 70% of the data for the training data, it means that more data we have, the better chances we have to predict future outcomes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWzU8xjms2na",
        "colab_type": "text"
      },
      "source": [
        "Q5. QUESTION #5: If your boss asked, \"How well does Census data, overall, predict sales?\" What would you say? Why?\n",
        "\n",
        "We would say that we need atleast 70% of the population to predict future sales accurately, and of that population, areas where women between 15 - 50 years who had a kid in the last 12 months and women between 30 - 34 years we are most likely to sell Bobo Bars. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzyDrE9htZEf",
        "colab_type": "code",
        "outputId": "b0299b47-ae58-43d7-cb95-307486bd2ad1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Finally, let's see what our y-intercept is, so we can interpret what our baseline sales number looks like, all things considered:\n",
        "print(\"y interecept:\")\n",
        "print(model.intercept_)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y interecept:\n",
            "22.19738813257551\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Miv6MJtJtTCx",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "QUESTION #6: What is our baseline sales number? What does that mean, practically? Think back to what y-intercepts mean in regression models.\n",
        "\n",
        "\n",
        "Our baseline sales number is 22.19738813257551\n",
        "\n",
        "The y- intercept means the expected value of Y when x is zero. \n",
        "In this case, it means that 22.19738813257551 is the starting point or the basic number of sales that Bobo Bars will have without any marketing efforts. Bobo bars can use this number as reference when as measuring reference for future sales. "
      ]
    }
  ]
}